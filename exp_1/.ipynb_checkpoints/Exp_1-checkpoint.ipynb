{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e0af5f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"insan öldürmenin cezası\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c426707c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gokhan Has\n",
    "\n",
    "import scipy\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ca90e44",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'SentenceTransformer' object has no attribute 'Dense'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-29c6d41aa170>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Install BERT model by using SentenceTransformer package.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSentenceTransformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'dbmdz/bert-base-turkish-128k-cased'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniforge3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    945\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    946\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 947\u001b[0;31m         raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n\u001b[0m\u001b[1;32m    948\u001b[0m             type(self).__name__, name))\n\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'SentenceTransformer' object has no attribute 'Dense'"
     ]
    }
   ],
   "source": [
    "# Install BERT model by using SentenceTransformer package.\n",
    "model = SentenceTransformer('dbmdz/bert-base-turkish-128k-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a156cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read dataset function ...\n",
    "def read_csv(filepath):\n",
    "     if os.path.splitext(filepath)[1] != '.csv':\n",
    "          return  # or whatever\n",
    "     seps = [';', '\\t']                    # ',' is default\n",
    "     encodings = [None, 'utf-8', 'ISO-8859-1', 'utf-16','ascii']  # None is default\n",
    "     for sep in seps:\n",
    "         for encoding in encodings:\n",
    "              try:\n",
    "                  return pd.read_csv(filepath, encoding=encoding, sep=sep)\n",
    "              except Exception:  # should really be more specific \n",
    "                  pass\n",
    "     raise ValueError(\"{!r} is has no encoding in {} or seperator in {}\"\n",
    "                      .format(filepath, encodings, seps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "255323ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SENTENCE\n",
      "LAW\n",
      "['SENTENCE', 'LAW']\n"
     ]
    }
   ],
   "source": [
    "# Get the dataset embeddings ...\n",
    "input_df = read_csv('../datasets/exp1_dataset.csv')\n",
    "input_df = input_df.head(4603)\n",
    "\n",
    "for col in input_df.columns: \n",
    "    print(col) \n",
    "\n",
    "print(input_df.columns.tolist())\n",
    "\n",
    "#sentences = list(input_df.groupby('SENTENCE').groups.items())\n",
    "sentences = input_df['SENTENCE'].values.tolist()\n",
    "\n",
    "sentence_embeddings = model.encode(sentences)\n",
    "\n",
    "print('Sample BERT embedding vector - length', len(sentence_embeddings[0]))\n",
    "\n",
    "print('Sample BERT embedding vector - note includes negative values', sentence_embeddings[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dac68409",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-c21618abd3e3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mqueries\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mquery_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqueries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Find the closest 3 sentences of the corpus for each query sentence based on cosine similarity\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "queries = [query]\n",
    "query_embeddings = model.encode(queries)\n",
    "\n",
    "# Find the closest 3 sentences of the corpus for each query sentence based on cosine similarity\n",
    "number_top_matches = 10 #@param {type: \"number\"}\n",
    "\n",
    "print(\"Anlamsal arama sonuçları\")\n",
    "\n",
    "for query, query_embedding in zip(queries, query_embeddings):\n",
    "    distances = scipy.spatial.distance.cdist([query_embedding], sentence_embeddings, \"cosine\")[0]\n",
    "\n",
    "    results = zip(range(len(distances)), distances)\n",
    "    results = sorted(results, key=lambda x: x[1])\n",
    "\n",
    "    print(\"\\n\\n======================\\n\\n\")\n",
    "    print(\"ARANAN CÜMLE:\", query)\n",
    "    print(\"\\nEN YÜKSEK 10 SONUÇ GÖSTERİLİYOR :\\n\")\n",
    "\n",
    "    for idx, distance in results[0:number_top_matches]:\n",
    "        print(sentences[idx].strip(), \"(Cosine Score: %.4f)\" % (1-distance))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b13c7d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
